LUE刷榜⼯具，可⽀持ChatGPT类模型训练和部署产品HugChat，以及统⼀信息抽取产品HugIE等。
HugNLP是⼀个分层式框架，遵循“⾼内聚低耦合”的开发模式，其核⼼包括模型层（Models）、处理器层（Processors）、评估器层（Evaluators）和应⽤层（Applications）四部分。
INSTRUCTEVALhttps://mp.weixin.qq.com/s/E6hq0AUy_hItA5HGo2tCAQhttps://github.com/declare-lab/instruct-evalhttps://arxiv.org/abs/2306.04757本⽂引⼊了⼀个名为INSTRUCTEVAL的新型评估套件。
该套件专⽤于对指令调优⼤型语⾔模型的全⾯评估，相⽐之前对LLMs的评估⽅法，该评估策略不仅详细评估了模型解决问题的能⼒、⽂字写作能⼒，⽽且还严格评估了模型与⼈类价值的对⻬能⼒。
LOw-Memory Optimization (LOMO)https://arxiv.org/abs/2306.09782https://github.com/OpenLMLab/LOMOLarge Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but demand massive GPU resources for training. Lowering the threshold for LLMs training would encourage greater participation from researchers, benefiting both academia and society. While existing approaches have focused on parameter-efficient fine-tuning, which tunes or adds a small number of parameters, few have addressed the challenge of tuning the full parameters of LLMs with limited resources. In this work, we propose a new optimizer, LOw-MemoryOptimization (LOMO), which fuses the gradient computation and the parameter update in one step to reduce memory usage. By integrating LOMO with existing memory saving techniques, we reduce memory usage to 10.8% compared to the standard approach (DeepSpeed solution). Consequently, our approach enables the full parameter fine-tuning of a 65B model on a single machine with 8 RTX 3090, each with24GB memory.llama.cpphttps://github.com/ggerganov/llama.cppInference of LLaMA model in pure C/C++The main goal is to run the model using 4-bit quantization on a MacBookPlain C/C++ implementation without dependenciesApple silicon first-class citizen - optimized via ARM NEONAVX2 support for x86 architecturesMixed F16 / F32 precision4-bit quantization supportRuns on the CPUMeZO: Fine-Tuning Language Models with Just Forward Passeshttps://github.com/princeton-nlp/MeZOhttps://arxiv.org/abs/2305.17333https://mp.weixin.qq.com/s/3RLCVQg2QJGSiDUtx9DgPgThis is the implementation for the paper Fine-Tuning Language Models with Just Forward Passes. In this paper we propose a memory-efficient zeroth-order optimizer (MeZO), adapting the classical zeroth-order SGD method to operate in-place, thereby fine-tuning language models (LMs) with the same memory footprint as inference.With a single A100 80GB GPU, MeZO can train a 30-billion parameter OPT model, whereas fine-tuning with Adam can train only a 2.7B LM.MeZO demonstrates comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12× memory reduction. MeZO is also compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning. We also show that MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1).MLC LLMhttps://github.com/mlc-ai/mlc-llmMLC LLM is a universal solution that allows any language models to be deployed natively on a diverse set of hardware backends and nativeapplications, plus a productive framework for everyone to further optimize model performance for their own use cases.Our mission is to enable everyone to develop, optimize and deploy AI models natively on everyone's devices.Everything runs locally with no server support and accelerated with local GPUs on your phone and laptops. Supported platforms include:iPhone, iPadMetal GPUs and Intel/ARM MacBooks;AMD, Intel and NVIDIA GPUs via Vulkan on Windows and Linux;NVIDIA GPUs via CUDA on Windows and Linux;WebGPU on browsers (through companion project WebLLM).PKU-Beaver 河狸 (Safe RLHF)https://github.com/PKU-Alignment/safe-rlhfhttps://mp.weixin.qq.com/s/ZpkgszXbisl5xf63EfTNjQ北京⼤学团队开源了名为 PKU-Beaver（河狸）项⽬，其开源地址为：https://github.com/PKU-Alignment/safe-rlhf。
该项⽬⾸次公开了 RLHF 所需的数据集、训练和验证代码，是⽬前⾸个开源的可复现的 RLHF 基准。
同时，为解决⼈类标注产⽣的偏⻅和歧视等不安全因素，北京⼤学团队⾸次提出了带有约束的价值对⻬技术 CVA（Constrained Value Alignment）。
该技术通过对标注信息进⾏细粒度划分，并结合带约束的安全强化学习⽅法，显著降低了模型的偏⻅和歧视，提⾼了模型的安全性。
Beaver使⽤GPT4进⾏Evaluation，结果表明，在原有性能保持不变的情况下，Beaver回复的安全性⼤幅度提升。
PaLM + RLHF (Pytorch)https://github.com/lucidrains/PaLM-rlhf-pytorchImplementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Maybe I'll add retrieval functionality too, à la RETRORL4LMshttps://github.com/allenai/RL4LMshttps://rl4lms.apps.allenai.org/A modular RL library to fine-tune language models to human preferencesWe provide easily customizable building blocks for training language models including implementations of on-policy algorithms, reward functions, metrics, datasets and LM based actor-critic policiesReinforcement Learning with Language Modelhttps://github.com/HarderThenHarder/transformers_tasks/tree/main/RLHF在这个项⽬中，我们将通过开源项⽬ trl 搭建⼀个通过强化学习算法（PPO）来更新语⾔模型（GPT-2）的⼏个示例，包括：基于中⽂情感识别模型的正向评论⽣成机器⼈（No Human Reward）基于⼈⼯打分的正向评论⽣成机器⼈（With Human Reward）基于排序序列（Rank List）训练⼀个奖励模型（Reward Model）排序序列（Rank List）标注平台SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compressionhttps://github.com/Vahe1994/SpQRhttps://arxiv.org/pdf/2306.03078.pdfhttps://mp.weixin.qq.com/s/819L-dY54BaVM1vub9OSpQSpQR 通过识别和隔离异常权重来⼯作，这些异常权重会导致特别⼤的量化误差，研究者将它们以更⾼的精度存储，同时将所有其他权重压缩到 3-4 位，在 LLaMA 和 Falcon LLMs 中实现了不到 1% 的困惑度相对准确率损失。
从⽽可以在单个 24GB 的消费级 GPU 上运⾏ 33B 参数的 LLM，⽽不会有任何性能下降，同时还能提⾼ 15% 的速度。
Scikit-LLM: Sklearn Meets Large Language Modelshttps://github.com/iryna-kondr/scikit-llmSeamlessly integrate powerful language models like ChatGPT into scikit-learn for enhanced text analysis tasks.Transformer Reinforcement Learninghttps://github.com/lvwerra/trlWith trl you can train transformer language models with Proximal Policy Optimization (PPO). The library is built on top of the transformers library by Hugging Face. Therefore, pre-trained language models can be directly loaded via transformers. At this point most of decoderarchitectures and encoder-decoder architectures are supported.$Train_Transformers_with_INT4https://mp.weixin.qq.com/s/pyEJJ5AvQqfyncO7CA8eNAhttps://arxiv.org/abs/2306.11987https://github.com/xijiu9/Train_Transformers_with_INT4Quantizing the activation, weight, and gradient to 4-bit is promising to accelerate neural network training. However, existing 4-bit trainingmethods require custom numerical formats which are not supported by contemporary hardware. In this work, we propose a training method for transformers with all matrix multiplications implemented with the INT4 arithmetic. Training with an ultra-low INT4 precision is challenging. To achieve this, we carefully analyze the specific structures of activation and gradients in transformers to propose dedicated quantizers for them. For forward propagation, we identify the challenge of outliers and propose a Hadamard quantizer to suppress the outliers. For backpropagation, we leverage the structural sparsity of gradients by proposing bit splitting and leverage score sampling techniques to quantize gradients accurately. Our algorithm achieves competitive accuracy on a wide range of tasks including natural language understanding,machine translation, and image classification. Unlike previous 4-bit training methods, our algorithm can be implemented on the current generation of GPUs. Our prototypical linear operator implementation is up to 2.2 times faster than the FP16 counterparts and speeds up the training by up to 35.1%.Transformer Reinforcement Learning Xhttps://github.com/CarperAI/trlxtrlX is a distributed training framework designed from the ground up to focus on fine-tuning large language models with reinforcement learning using either a provided reward function or a reward-labeled dataset.Training support for Hugging Face models is provided by Accelerate-backed trainers, allowing users to fine-tune causal and T5-based language models of up to 20B parameters, such as facebook/opt-6.7b, EleutherAI/gpt-neox-20b, and google/flan-t5-xxl. For models beyond 20B parameters, trlX provides NVIDIA NeMo-backed trainers that leverage efficient parallelism techniques to scale effectively.vLLMhttps://github.com/vllm-project/vllmvLLM is a fast and easy-to-use library for LLM inference and serving.vLLM is fast with:State-of-the-art serving throughputEfficient management of attention key and value memory with PagedAttentionDynamic batching of incoming requestsOptimized CUDA kernelsvLLM is flexible and easy to use with:Seamless integration with popular HuggingFace modelsHigh-throughput serving with various decoding algorithms, including parallel sampling, beam search, and moreTensor parallelism support for distributed inferenceStreaming outputsOpenAI-compatible API server3 可参考的其它开源模型Cerebras（可商⽤）https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/https://huggingface.co/cerebras开源7个可商⽤GPT模型，含数据集和可直接下载的预训练模型权重: Cerebras 开源 7 个 GPT 模型，均可商⽤，参数量分别达到 1.11 亿、2.56 亿、5.9 亿、13 亿、27 亿、67 亿和 130 亿。
其中最⼤的模型参数量达到 130 亿，与 Meta 最近开源的 LLaMA-13B 相当。
该项⽬开源数据集和预训练模型权重，其中预训练模型权重⽂件⼤⼩近50G可直接下载，并且可⽤于商业和研究⽤途。
与此前的 GPT-3 模型相⽐，Cerebras开源的模型具有更⾼的可⽤性和透明度，研究⼈员和开发者可以使⽤少量数据对其进⾏微调，构建出⾼质量的⾃然语⾔处理应⽤。
ChatDoctor$https://github.com/Kent0n-Li/ChatDoctorhttps://arxiv.org/abs/2303.14070Recent large language models (LLMs) in the general domain, such as ChatGPT, have shown remarkable success in following instructions and producing human-like responses. However, such language models have yet to be adapted for the medical domain, resulting in poor accuracy of responses and an inability to provide sound advice on medical diagnoses, medications, etc. To address this problem, we fine-tuned our ChatDoctor model based on 100k real-world patient-physician conversations from an online medical consultation site. Besides, we add autonomous knowledge retrieval capabilities to our ChatDoctor, for example, Wikipedia or a disease database as a knowledge brain. Byfine-tuning the LLMs using these 100k patient-physician conversations, our model showed significant improvements in understanding patients' needs and providing informed advice. The autonomous ChatDoctor model based on Wikipedia and Database Brain can access real-time and authoritative information and answer patient questions based on this information, significantly improving the accuracy of the model's responses, which shows extraordinary potential for the medical field with a low tolerance for error.Dolly 1&2（可商⽤）https://github.com/databrickslabs/dollyhttps://huggingface.co/databricks/dolly-v2-12bhttps://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.htmlWe show that anyone can take a dated off-the-shelf open source large language model (LLM) and give it magical ChatGPT-like instructionfollowing ability by training it in 30 minutes on one machine, using high-quality training data. Surprisingly, instruction-following does not seem to require the latest or largest models: our model is only 6 billion parameters, compared to 175 billion for GPT-3. We open source the code for our model (Dolly) and show how it can be re-created on Databricks. We believe models like Dolly will help democratize LLMs, transforming them from something very few companies can afford into a commodity every company can own and customize to improve their products.FinGPThttps://github.com/ai4finance-foundation/fingpthttps://arxiv.org/pdf/2306.06031v1.pdfhttps://mp.weixin.qq.com/s/A9euFin675nxGGciiX6rJQLarge language models (LLMs) have shown the potential of revolutionizing natural language processing tasks in diverse domains, sparkinggreat interest in finance. Accessing high-quality financial data is the first challenge for financial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls for an open-source alternative todemocratize Internet-scale financial data.In this paper, we present an open-source large language model, FinGPT, for the finance sector. Unlike proprietary models, FinGPT takes adata-centric approach, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. We highlight the importance of an automatic data curation pipeline and the lightweight low-rank adaptation technique in building FinGPT. Furthermore, we showcase several potential applications as stepping stones for users, such as robo-advising, algorithmic trading, and low-codedevelopment. Through collaborative efforts within the open-source AI4Finance community, FinGPT aims to stimulate innovation, democratize FinLLMs, and unlock new opportunities in open finance.Falcon（可商⽤）https://mp.weixin.qq.com/s/mKx0ZiTB28khj4U7EVJiVwhttps://falconllm.tii.ae/https://huggingface.co/tiiuae/falcon-40bFalcon LLM is a foundational large language model (LLM) with 40 billion parameters trained on one trillion tokens. TII has now released Falcon LLM – a 40B model.The model uses only 75 percent of GPT-3’s training compute, 40 percent of Chinchilla’s, and 80 percent of PaLM-62B’s.Facebook/Meta LLaMA/LLaMA2https://github.com/facebookresearch/llamahttps://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/LLaMA1LLaMA: Open and Efficient Foundation Language ModelsWe introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.LLaMA2We are unlocking the power of large language models. Our latest version of Llama is now accessible to individuals, creators, researchers and businesses of all sizes so that they can experiment, innovate and scale their ideas responsibly.This release includes model weights and starting code for pretrained and fine-tuned Llama language models — ranging from 7B to 70B parameters.This repository is intended as a minimal example to load Llama 2 models and run inference. For more detailed examples leveraging HuggingFace, see llama-recipes.GALACTICAhttps://github.com/paperswithcode/galaihttps://arxiv.org/pdf/2211.09085.pdfhttps://galactica.org/GALACTICA is a general-purpose scientific language model. It is trained on a large corpus of scientific text and data. It can perform scientific NLP tasks at a high level, as well as tasks such as citation prediction, mathematical reasoning, molecular property prediction and proteinannotation. More information is available at galactica.org.Goar-7B for Arithmetic Taskshttps://mp.weixin.qq.com/s/_haINkHNV4bMszm9F41yXAhttps://arxiv.org/pdf/2305.14201.pdfhttps://github.com/liutiedong/goat在本⽂介绍了⼀种微调的语⾔模型：Goat。
不同于以往对算术计算的研究，该模型在 LLaMA上采⽤端到端监督指令微调范式，利⽤包含约100万个样本的综合⽣成数据集进⾏训练得到。
它⾮常擅⻓算术任务。
Goat 在初等算术（包括整数的加法、减法、乘法和除法）中实现了最先进的性能。
实验结果表明，仅通过监督微调⽽不应⽤任何特殊技术，「Goat模型能够在Zero-shot设置中以近乎完美的精度为⼤数加法和减法⽣成答案」。
这种出⾊的算术能⼒归因于 LLaMA 对数字的⼀致标记化，并表明这对于以前的 LLM 来说⼏乎是不可能实现的，例如 Bloom、OPT、GPT-NeoX 、Pythia等。
然⽽，该模型在⾯对乘除运算时遇到了很⼤的挑战。
为了克服这⼀挑战，本⽂提出了⼀种⽅法，即「将各种算术任务分为可学习和不可学习任务」，随后利⽤基本算术原理将不可学习任务（例如多位数乘法和除法）分解为⼀系列可学习任务。
本⽂⽅法确保促进模型学习的中间监督也很容易被⼈类理解，即通过模型微调在⽣成最终答案之前⽣成合适的CoT。
「本⽂⽅法⼤⼤优于 GPT-4 的⻓乘法和⻓除法」。
最终使⽤ BIG-bench (Srivastava et al., 2022) 算术⼦任务评估模型的性能，并对本⽂⽅法的有效性进⾏综合评估。
实验结果表明，该模型可以学习计算模式并将其泛化到看不⻅的数据，⽽不仅仅是纯粹权重记忆计算。
此外，Goat-7B 可以在24GB VRAM GPU上使⽤LoRA低秩适应技术进⾏训练，可以「很容易复现论⽂成果」。
HuggingChathttps://huggingface.co/chat/Making the community's best AI chat models available to everyone.Koala: A Dialogue Model for Academic Researchhttps://bair.berkeley.edu/blog/2023/04/03/koala/In this post, we introduce Koala, a chatbot trained by fine-tuning Meta’s LLaMA on dialogue data gathered from the web. We describe thedataset curation and training process of our model, and also present the results of a user study that compares our model to ChatGPT andStanford’s Alpaca. Our results show that Koala can effectively respond to a variety of user queries, generating responses that are often preferred over Alpaca, and at least tied with ChatGPT in over half of the cases.LongLLaMAhttps://mp.weixin.qq.com/s/XzaET7WfrNpOf-zdiSxrighttps://arxiv.org/pdf/2307.03170.pdfhttps://github.com/CStanKonrad/long_llamahttps://huggingface.co/syzymon/long_llama_3bThis repository contains the research preview of LongLLaMA, a large language model capable of handling long contexts of 256k tokens oreven more.LongLLaMA is built upon the foundation of OpenLLaMA and fine-tuned using the Focused Transformer (FoT) method. We release a smaller3B variant of the LongLLaMA model on a permissive license (Apache 2.0) and inference code supporting longer contexts on Hugging Face.Our model weights can serve as the drop-in replacement of LLaMA in existing implementations (for short context up to 2048 tokens). Additionally, we provide evaluation results and comparisons against the original OpenLLaMA models. Stay tuned for further updates.LLaMA复刻版OpenLLaMAhttps://github.com/openlm-research/open_llamaIn this repo, we release a permissively licensed open source reproduction of Meta AI's LLaMA large language model. In this release, we're releasing a public preview of the 7B OpenLLaMA model that has been trained with 200 billion tokens. We provide PyTorch and Jax weightsof pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. Stay tuned for our updates.Llama-X: Open Academic Research on Improving LLaMA to SOTA LLMhttps://github.com/AetherCortex/Llama-XThis is the repo for the Llama-X, which aims to:Progressively improve the performance of LLaMA to SOTA LLM with open-source community.Conduct Llama-X as an open academic research which is long-term, systematic and rigorous.Save the repetitive work of community and we work together to create more and faster increment.Lit-LLaMAhttps://github.com/Lightning-AI/lit-llamaLit-LLaMA is:Simple: Single-file implementation without boilerplate.Correct: Numerically equivalent to the original model.Optimized: Runs on consumer hardware or at scale.Open-source: No strings attached.MPT-7B（可商⽤）https://www.mosaicml.com/blog/mpt-7bhttps://huggingface.co/mosaicml/mpt-7bMPT-7B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code. This model was trained by MosaicML.MPT-7B is part of the family of MosaicPretrainedTransformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.Introducing MPT-7B, the latest entry in our MosaicML Foundation Series. MPT-7B is a transformer trained from scratch on 1T tokens of text and code. It is open source, available for commercial use, and matches the quality of LLaMA-7B. MPT-7B was trained on the MosaicML platform in 9.5 days with zero human intervention at a cost of ~$200k. Starting today, you can train, finetune, and deploy your own private MPT models, either starting from one of our checkpoints or training from scratch. For inspiration, we are also releasing three finetuned models in addition to the base MPT-7B: MPT-7B-Instruct, MPT-7B-Chat, and MPT-7B-StoryWriter-65k+, the last of which uses a context lengthof 65k tokens!OpenGPThttps://github.com/CogStack/OpenGPTA framework for creating grounded instruction based datasets and training conversational domain expert Large Language Models (LLMs).NHS-LLM：A conversational model for healthcare trained using OpenGPT. All the medical datasets used to train this model were created using OpenGPT and are available below.Orcahttps://aka.ms/orca-lmhttps://arxiv.org/pdf/2306.02707.pdfhttps://mp.weixin.qq.com/s/RRdrSeI2ux5QE6MqJ8opSgRecent research has focused on enhancing the capability of smaller models through imitation learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from limited imitation signals from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting i