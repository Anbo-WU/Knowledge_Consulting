域专有词表、⼤规模中⽂法律语料预训练，增强了⼤模型在法律领域的基础语义理解能⼒。
在此基础上，构造法律领域对话问答数据集、中国司法考试数据集进⾏指令精调，提升了模型对法律内容的理解和执⾏能⼒。
Lawyer LLaMAhttps://github.com/AndrewZhe/lawyer-llamaLawyer LLaMA ⾸先在⼤规模法律语料上进⾏了continual pretraining，让它系统的学习中国的法律知识体系。
 在此基础上，我们借助ChatGPT收集了⼀批对中国国家统⼀法律职业资格考试客观题（以下简称法考）的分析和对法律咨询的回答，利⽤收集到的数据对模型进⾏指令微调，让模型习得将法律知识应⽤到具体场景中的能⼒。
我们的模型能够：掌握中国法律知识： 能够正确的理解⺠法、刑法、⾏政法、诉讼法等常⻅领域的法律概念。
例如，掌握了刑法中的犯罪构成理论，能够从刑事案件的事实描述中识别犯罪主体、犯罪客体、犯罪⾏为、主观⼼理状态等犯罪构成要件。
模型利⽤学到的法律概念与理论，能够较好回答法考中的⼤部分题⽬。
应⽤于中国法律实务：能够以通俗易懂的语⾔解释法律概念，并且进⾏基础的法律咨询，涵盖婚姻、借贷、海商、刑事等法律领域。
为了给中⽂法律⼤模型的开放研究添砖加瓦，本项⽬将开源⼀系列法律领域的指令微调数据和基于LLaMA训练的中⽂法律⼤模型的参数。
LexiLawhttps://github.com/CSHaitao/LexiLawLexiLaw 是⼀个经过微调的中⽂法律⼤模型，它基于 ChatGLM-6B 架构，通过在法律领域的数据集上进⾏微调，使其在提供法律咨询和⽀持⽅⾯具备更⾼的性能和专业性。
该模型旨在为法律从业者、学⽣和普通⽤户提供准确、可靠的法律咨询服务。
⽆论您是需要针对具体法律问题的咨询，还是对法律条款、案例解析、法规解读等⽅⾯的查询，LexiLaw 都能够为您提供有益的建议和指导。
同时，我们将分享在⼤模型基础上微调的经验和最佳实践，以帮助社区开发更多优秀的中⽂法律⼤模型，推动中⽂法律智能化的发展。
LawGPT_zh 中⽂法律⼤模型（獬⾘）https://mp.weixin.qq.com/s/Pk4NdFQq5G6iZ3QmcyyFUghttps://github.com/LiuHC0428/LAW-GPT我们的愿景是为让所有⼈在遇到法律问题时能第⼀时间获得专业可靠的回答。
因为专业的律师服务只有真正触⼿可及，才会让⼈们习惯运⽤，⼀如⼆⼗年前的搜索引擎，⼗年前的快递业务。
我们希望让法律⾛进⽇常⽣活，为构建法治社会贡献我们的⼒量。
项⽬海报由Midjourney⽣成。
本项⽬开源的中⽂法律通⽤模型由ChatGLM-6B LoRA 16-bit指令微调得到。
数据集包括现有的法律问答数据集和基于法条和真实案例指导的self-Instruct构建的⾼质量法律⽂本问答，提⾼了通⽤语⾔⼤模型在法律领域的表现，提⾼了模型回答的可靠性和专业程度。
Linly伶荔说https://github.com/CVI-SZU/Linlyhttps://mp.weixin.qq.com/s/zSxsArP1pxYNubNDZua7iA“伶荔说”模型具有以下优势：1. 在32*A100 GPU上训练了不同量级和功能的中⽂模型，对模型充分训练并提供强⼤的baseline。
据我们所知33B的Linly-Chinese-LLAMA是⽬前最⼤的中⽂LLaMA模型。
2. 公开所有训练数据、代码、参数细节以及实验结果，确保项⽬的可复现性，⽤户可以选择合适的资源直接⽤于⾃⼰的流程中。
3. 项⽬具有⾼兼容性和易⽤性，提供可⽤于CUDA和CPU的量化推理框架，并⽀持Huggingface格式。
⽬前公开可⽤的模型有：Linly-Chinese-LLaMA：中⽂基础模型，基于LLaMA在⾼质量中⽂语料上增量训练强化中⽂语⾔能⼒，现已开放 7B、13B 和 33B 量级，65B正在训练中。
Linly-ChatFlow：中⽂对话模型，在400万指令数据集合上对中⽂基础模型指令精调，现已开放7B、13B对话模型。
Linly-ChatFlow-int4 ：ChatFlow 4-bit量化版本，⽤于在CPU上部署模型推理。
进⾏中的项⽬： Linly-Chinese-BLOOM：基于BLOOM中⽂增量训练的中⽂基础模型，包含7B和175B模型量级，可⽤于商业场景。
Linly伶荔说-Chinese-Falconhttps://mp.weixin.qq.com/s/AuAG3tw4JI8lHyLkSdM18ghttps://github.com/CVI-SZU/Linly近期，阿联酋阿布扎⽐的技术创新研究所（TII）开源了 Falcon 系列模型，使⽤经过筛选的 1 万亿 tokens 进⾏预训练，并以 Apache 2.0 协议开源，可能是⽬前效果最好且许可协议最宽松（允许商⽤）的开源模型。
然⽽，Falcon 模型在使⽤上⾯临和 LLaMA 模型类似的问题：由于模型主要在英⽂数据集上训练，因此它理解和⽣成中⽂的能⼒偏弱。
此外，Falcon 在构建词表时没有加⼊中⽂字/词，中⽂字会被拆分成多个 token 的组合，这导致中⽂⽂本会被拆分成更⻓的 tokens 序列，降低了编码和⽣成效率。
针对以上问题，“伶荔（Linly）”项⽬团队以 Falcon 模型为底座扩充中⽂词表，利⽤中⽂和中英平⾏增量预训练将模型的语⾔能⼒迁移学习到中⽂，实现 Chinese-Falcon。
本⽂从模型结构上分析 Falcon、LLaMA 与传统 GPT 的异同，代码实现细节。
并介绍我们的中⽂ Falcon 训练⽅案，包括中⽂字词扩充、数据集构建和训练参数等。
MeChat (Mental Health Support Chatbot)https://github.com/qiuhuachuan/smilehttps://huggingface.co/qiuhuachuan/MeChathttps://mechat.fly.dev/我们的愿景是为让所有⼈在遇到⼼理健康问题时能够获得及时、有效的倾听和⽀持。
我们相信，⼼理健康是每个⼈的权利，⽽不是奢侈品。
我们的使命是为⼈们提供平等、全⾯、易于访问的⼼理健康服务，⽆论他们身在何处、⾯临何种挑战。
我们的愿景还包括推动社会对⼼理健康问题的认识和理解，打破⼼理健康问题带来的污名和歧视，为创建⼀个更加健康、包容和平等的社会做出贡献。
项⽬海报取⾃ flaticon 。
MedicalGPThttps://github.com/shibing624/MedicalGPTMedicalGPT 训练医疗⼤模型，实现包括⼆次预训练、有监督微调、奖励建模、强化学习训练。
基于ChatGPT Training Pipeline，本项⽬实现了领域模型--医疗模型的四阶段训练：第⼀阶段：PT(Continue PreTraining)增量预训练，在海量领域⽂档数据上⼆次预训练GPT模型，以注⼊领域知识第⼆阶段：SFT(Supervised Fine-tuning)有监督微调，构造指令微调数据集，在预训练模型基础上做指令精调，以对⻬指令意图第三阶段：RM(Reward Model)奖励模型建模，构造⼈类偏好排序数据集，训练奖励模型，⽤来对⻬⼈类偏好，主要是"HHH"原则，具体是"helpful, honest, harmless"第四阶段：RL(Reinforcement Learning)基于⼈类反馈的强化学习(RLHF)，⽤奖励模型来训练SFT模型，⽣成模型使⽤奖励或惩罚来更新其策略，以便⽣成更⾼质量、更符合⼈类偏好的⽂本MedicalGPT-zhgithub.com/MediaBrain-SJTU/MedicalGPT-zh该开源了基于ChatGLM-6B LoRA 16-bit指令微调的中⽂医疗通⽤模型。
基于共计28科室的中⽂医疗共识与临床指南⽂本，我们⽣成医疗知识覆盖⾯更全，回答内容更加精准的⾼质量指令数据集。
OpenKG-KnowLLMhttps://github.com/zjunlp/KnowLLMKnowledgable Large Language Model Series.With the rapid development of deep learning technology, large language models such as ChatGPT have achieved significant success in thefield of natural language processing. However, these large models still face some challenges and issues in learning and understanding knowledge, including the difficulty of knowledge updating, and issues with potential errors and biases within the model, known as knowledge fallacies. The Deep Model series aims to release a series of open-source large models to mitigate these knowledge fallacy issues. The firstphase of this project released a knowledge extraction large model based on LLaMA, named Zhishi. To provide Chinese capabilities withoutdisrupting the original model's distribution, we firstly (1) use Chinese corpora for the full-scale pre-training of LLaMA (13B), in order to improve the model's understanding of Chinese and knowledge reserve as much as possible while retaining its original English and code capabilities; Then (2) we fine-tune the model from the first step using an instruction dataset, to enhance the language model's understanding ofhuman extraction instructions.OpenMEDLab 浦医https://github.com/OpenMEDLabhttps://github.com/openmedlab/PULSEhttps://stcsm.sh.gov.cn/xwzx/kjzl/20230630/c783c30d8e62494e83073535f841675f.htmlOpenMEDLab is an open-source platform to share medical foundation models in multi-modalities, e.g., medical imaging, medical NLP, bioinformatics, protein, etc. It targets promoting novel approaches to long-tail problems in medicine, and meanwhile, it seeks solutions to achieve lower cost, higher efficiency, and better generalizability in training medical AI models. The new learning paradigm of adapting foundation models to downstream applications makes it possible to develop innovative solutions for cross-domain and cross-modality diagnostic tasks efficiently. OpenMEDLab is distinguished by several features:World's first open-source platform for medical foundation models.10+ medical data modalities targeting a variety of clinical and research problems.Pioneering works of the new learning paradigm using foundation models, including pre-trained models, code, and data.Releasing multiple sets of medical data for pre-training and downstream applications.Collaboration with top medical institutes and facilities.PromptCLUEhttps://github.com/clue-ai/PromptCLUEPromptCLUE：⼤规模多任务Prompt预训练中⽂开源模型。
中⽂上的三⼤统⼀：统⼀模型框架，统⼀任务形式，统⼀应⽤⽅式。
⽀持⼏⼗个不同类型的任务，具有较好的零样本学习能⼒和少样本学习能⼒。
针对理解类任务，如分类、情感分析、抽取等，可以⾃定义标签体系；针对⽣成任务，可以进⾏采样⾃由⽣成。
千亿中⽂token上⼤规模预训练，累计学习1.5万亿中⽂token，亿级中⽂任务数据上完成训练，训练任务超过150+。
⽐base版平均任务提升7个点+；具有更好的理解、⽣成和抽取能⼒，并且⽀持⽂本改写、纠错、知识图谱问答。
SkyText-Chinese-GPT3https://github.com/SkyWorkAIGC/SkyText-Chinese-GPT3SkyText是由奇点智源发布的中⽂GPT3预训练⼤模型，可以进⾏聊天、问答、中英互译等不同的任务。
 应⽤这个模型，除了可以实现基本的聊天、对话、你问我答外，还能⽀持中英⽂互译、内容续写、对对联、写古诗、⽣成菜谱、第三⼈称转述、创建采访问题等多种功能。
ShenNong-TCM-LLMhttps://github.com/michael-wzhu/ShenNong-TCM-LLM为推动LLM在中医药领域的发展和落地，提升LLM的在中医药⽅⾯的知识与回答医学咨询的能⼒，同时推动⼤模型赋能中医药传承，我们现推出ShenNong中医药⼤规模语⾔模型:ShenNong-TCM :这⼀模型的训练数据为中医药指令数据集ShenNong_TCM_Dataset。
ChatMed_TCM_Dataset以我们开源的中医药知识图谱为基础；采⽤以实体为中⼼的⾃指令⽅法entity-centric self-instruct，调⽤ChatGPT得到11w+的围绕中医药的指令数据；ShenNong-TCM模型也是以LlaMA为底座，采⽤LoRA (rank=16)微调得到。
微调代码与ChatMed代码库相同TableGPThttps://github.com/ZJU-M3/TableGPT-techreportTableGPT is a specifically designed for table analysis. By unifying tables, natural language, and commands into one model, TableGPT comprehends tabular data, understands user intent through natural language, dissects the desired actions, and executes external commands onthe table. It subsequently returns the processed results in both tabular and textual explanations to the user. This novel approach simplifiesthe way users engage with table data, bringing an intuitive feel to data analysis.TechGPThttps://mp.weixin.qq.com/s/nF1He7jhAHfh7PzhjqHoZghttps://huggingface.co/neukg/TechGPT-7Bhttps://github.com/neukg/TechGPT2023年6⽉26⽇，“东北⼤学知识图谱研究组”正式发布⼤语⾔模型TechGPT。
!TechGPT的名字主要来源于⼩组在2018年推出的TechKG⼤规模中⽂学术多领域的知识库。
与当前其他各类⼤模型相⽐，TechGPT主要强化了以“知识图谱构建”为核⼼的关系三元组抽取等各类信息抽取任务、以“逻辑推理”为核⼼的机器阅读理解等各类智能问答任务、以“⽂本理解”为核⼼的关键词⽣成等各类序列⽣成任务。
在这三⼤⾃然语⾔处理核⼼能⼒之内，TechGPT还具备了对计算机科学、材料、机械、冶⾦、⾦融和航空航天等⼗余种垂直专业领域⾃然语⾔⽂本的处理能⼒。
TigerBothttps://github.com/TigerResearch/TigerBotTigerBot 是⼀个多语⾔多任务的⼤规模语⾔模型(LLM)。
根据 OpenAI InstructGPT 论⽂在公开 NLP 数据集上的⾃动评测，TigerBot-7B 达到OpenAI 同样⼤⼩模型的综合表现的 96%，并且这只是我们的 MVP，在此我们将如下探索成果开源：模型：TigerBot-7B, TigerBot-7B-base，TigerBot-180B (research version)，代码：基本训练和推理代码，包括双卡推理 180B 模型的量化和推理代码，数据：预训练 100G，从 2TB 过滤后的数据中经过去噪去重清洗⽽得；监督微调 1G 或 100 万条数据，按⽐例涵盖⽤户指令常⻅的 10 ⼤类 120 ⼩类任务，API: chat, plugin, finetune, 让⽤户能在半⼩时内⽆代码的训练和使⽤专属于⾃⼰的⼤模型和数据，领域数据：涵盖⾦融，法律，百科，⼴邀⼤模型应⽤开发者，⼀起打造中国的世界级的应⽤。
我们在 BLOOM 基础上，在模型架构和算法上做了如下优化：指令完成监督微调的创新算法以获得更好的可学习型(learnability)，运⽤ ensemble 和 probabilistic modeling 的⽅法实现更可控的事实性(factuality)和创造性(generativeness)，在并⾏训练上，我们突破了 deep-speed 等主流框架中若⼲内存和通信问题，使得在千卡环境下数⽉⽆间断，对中⽂语⾔的更不规则的分布，从 tokenizer 到训练算法上做了更适合的算法优化。
YuLan-Chathttps://github.com/RUC-GSAI/YuLan-Chathttps://mp.weixin.qq.com/s/nPS4N3stAAG_51fnZANbMA中国⼈⺠⼤学⾼瓴⼈⼯智能学院相关研究团队（由多位学院⽼师联合指导）展开了⼀系列关于指令微调技术的研究，并发布了学院初版⼤语⾔对话模型⸺YuLan-Chat，旨在探索和提升⼤语⾔模型的中英⽂双语对话能⼒。
我们分别开源了13B和65B的YuLan-Chat模型⽂件及相关代码，并采⽤量化技术使其分别可以在单张RTX3090-24G和A800-80G显卡上部署。
YuLan-Chat模型基于LLaMA底座模型，采⽤精⼼优化的⾼质量中英⽂混合指令进⾏微调，其中YuLan-Chat-65B模型⽬前能够在中英⽂相关评测数据集上显著超越已有开源模型效果。
后续我们会继续优化指令微调⽅法与底座模型，持续更新YuLan-Chat模型。
Ziya-LLaMAhttps://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1https://github.com/IDEA-CCNL/Fengshenbang-LMhttps://mp.weixin.qq.com/s/IeXgq8blGoeVbpIlAUCAjA姜⼦⽛通⽤⼤模型V1是基于LLaMa的130亿参数的⼤规模预训练模型，具备翻译，编程，⽂本分类，信息抽取，摘要，⽂案⽣成，常识问答和数学计算等能⼒。
⽬前姜⼦⽛通⽤⼤模型已完成⼤规模预训练、多任务有监督微调和⼈类反馈学习三阶段的训练过程。
The Ziya-LLaMA-13B-v1 is a large-scale pre-trained model based on LLaMA with 13 billion parameters. It has the ability to perform tasks such as translation, programming, text classification, information extraction, summarization, copywriting, common sense Q&A, and mathematical calculation. The Ziya-LLaMA-13B-v1 has undergone three stages of training: large-scale continual pre-training (PT), multi-task supervised fine-tuning (SFT), and human feedback learning (RM, PPO).2 训练/推理⾼效对⻬算法RAFT「⽊筏」https://github.com/OptimalScale/LMFlowhttps://arxiv.org/abs/2304.06767https://optimalscale.github.io/LMFlow/examples/raft.htmlAn extensible, convenient, and efficient toolbox for finetuning large machine learning models, designed to be user-friendly, speedy and reliable, and accessible to the entire community.Alpaca-LoRAhttps://github.com/tloen/alpaca-loraLow-Rank LLaMA Instruct-TuningThis repository contains code for reproducing the Stanford Alpaca results using low-rank adaptation (LoRA). We provide an Instruct modelof similar quality to text-davinci-003 that can run on a Raspberry Pi (for research), and the code can be easily extended to the 13b, 30b, and 65b models.In addition to the training code, which runs within five hours on a single RTX 4090, we publish a script for downloading and inference on the foundation model and LoRA, as well as the resulting LoRA weights themselves. To fine-tune cheaply and efficiently, we use Hugging Face's PEFT as well as Tim Dettmers' bitsandbytes.Without hyperparameter tuning or validation-based checkpointing, the LoRA model produces outputs comparable to the Stanford Alpacamodel. (Please see the outputs included below.) Further tuning might be able to achieve better performance; I invite interested users to give it a try and report their results.AlpacaFarmhttps://mp.weixin.qq.com/s/CIF2F5Vx_RSN1-LwU_ppOQhttps://tatsu-lab.github.io/alpaca_farm_paper.pdfhttps://github.com/tatsu-lab/alpaca_farm主流的⼤型语⾔模型训练都离不开RLHF(⼈⼯反馈强化学习)，其主要思想是使⽤⼈类专家提供的反馈示例来指导模型的学习过程，它可以加速强化学习过程，提⾼⼤模型的性能，但「⽬前RLHF这个过程既复杂⼜昂贵」。
针对RLHF这个问题，学术界⽬前主要有两种解决⽅法：「1）避开RLHF」，⽐如Meta最近研究的“Meta最新模型：LIMA-65B，没有RLHF，模型效果远胜Alpaca！！”，验证了精⼼制作的少量标注数据同样能达到不错的效果。
2）「简化RLHF」，就是今天给⼤家分享的这篇⽂章：斯坦福发布了⼀个名为AlpacaFarm（⽺驼农场）的模拟器，旨在降低训练语⾔模型的成本，且⽐⼈⼯成本低45倍，并表现出与⼈类反馈的⾼度⼀致性，同时也为RLHF的研究开辟了新的道路。
ColossalAIhttps://github.com/hpcaitech/ColossalAIColossal-AI: Making large AI models cheaper, faster and more accessibleColossal-AI provides a collection of parallel components for you. We aim to support you to write your distributed deep learning models justlike how you write your model on your laptop. We provide user-friendly tools to kickstart distributed training and inference in a few lines.ChatLLaMAhttps://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllamaChatLLaMA has been designed to help developers with various use cases, all related to RLHF training and optimized inference.ChatLLaMA is a library that allows you to create hyper-personalized ChatGPT-like assistants using your own data and the least amount of compute possible. Instead of depending on one large assistant that “rules us all”, we envision a future where each of us can create our ownpersonalized version of ChatGPT-like assistants. Imagine a future where many ChatLLaMAs at the "edge" will support a variety of human'sneeds. But creating a personalized assistant at the "edge" requires huge optimization efforts on many fronts: dataset creation, efficient training with RLHF, and inference optimization.Chinese-Guanacohttps://github.com/jianzhnie/Chinese-GuanacoThis is the repo for the Chinese-Guanaco project, which aims to build and share instruction-following Chinese LLaMA/Pythia/GLM model tuning methods which can be trained on a single Nvidia RTX-2080TI, multi-round chatbot which can be trained on a single Nvidia RTX-3090with the context len 2048.Chinese-Guanaco uses bitsandbytes for quantization and is integrated with Huggingface's PEFT and transformers libraries.#DPO (Direct Preference Optimization)https://arxiv.org/abs/2305.18290https://zhuanlan.zhihu.com/p/641045324https://huggingface.co/lyogavin/Anima33B-DPO-Belle-1k-mergedhttps://github.com/lyogavin/Anima/tree/main/rlhfDPO的核⼼原理是：PPO训练难度核⼼是因为需要通过reward model来表达偏好，进⾏强化学习。
为了不再依赖于reward model进⾏强化学习，他进⾏了⼀系列的数学变换，直接推导出了基于Policy Language Model的标注偏好的概率表达形式，从⽽可以直接求解⼀个Language Model的最⼤似然估计。
不再需要复杂繁琐的reward model和强化学习。
While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise controlof their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often withreinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper, we leverage a mapping between reward functions and optimalpolicies to show that this constrained reward maximization problem can be optimized exactly with a single stage of policy training, essentially solving a classification problem on the human preference data. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant and computationally lightweight, eliminating the need for fitting a reward model, sampling from the LM during fine-tuning, or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds RLHF's ability to control sentiment of generations and improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.DialogADV：Evaluate What You Can't Evaluate: Unassessable Generated Responses Qualityhttps://github.com/misonsky/DialogADVhttps://mp.weixin.qq.com/s/Ga0a6a1L6CmCXgk6WDz0Xghttps://arxiv.org/abs/2305.14658我们构建了两个具有挑战的元验证对话数据集，通过实验分析表明⼤型语⾔模型作为评估器评估对话⽂本⽣成质量仍然存在很多问题：1）LLMs⽆法识别与事实不⼀致的、虚构的回复，对不合理的回复仍然给出较⾼的评价；2） LLMs⾃身的知识有限，对于依赖知识的样例⼤语⾔模型⽆法依靠⾃身的知识给出合理的判断；3）LLMs利⽤外部知识的能⼒有待提⾼。
在给定外部知识的情况下，LLMs仍然会对不合理的回复给出较⾼的评价。
DeepSpeed-Chathttps://mp.weixin.qq.com/s/t3HA4Hu61LLDC3h2Njmo_Qhttps://github.com/microsoft/DeepSpeed微软宣布开源 DeepSpeed-Chat，帮助⽤户轻松训练类 ChatGPT 等⼤语⾔模型。
据悉，Deep Speed Chat 是基于微软 Deep Speed 深度学习优化库开发⽽成，具备训练、强化推理等功能，还使⽤了 RLHF（基于⼈类反馈的强化学习）技术，可将训练速度提升 15 倍以上，⽽成本却⼤⼤降低。
FlexGenhttps://github.com/FMInference/FlexGenFlexGen is a high-throughput generation engine for running large language models with limited GPU memory. FlexGen allows high-throughput generation by IO-efficient offloading, compression, and large effective batch sizes.Limitation. As an offloading-based system running on weak GPUs, FlexGen also has its limitations. FlexGen can be significantly slower thanthe case when you have enough powerful GPUs to hold the whole model, especially for small-batch cases. FlexGen is mostly optimized forthroughput-oriented batch processing settings (e.g., classifying or extracting information from many documents in batches), on single GPUs.FlagAI and FlagDatahttps://github.com/FlagAI-Open/FlagAIFlagAI (Fast LArge-scale General AI models) is a fast, easy-to-use and extensible toolkit for large-scale model. Our goal is to support training, fine-tuning, and deployment of large-scale models on various downstream tasks with multi-modality.https://github.com/FlagOpen/FlagDataFlagData, a data processing toolkit that is easy to use and expand. FlagData integrates the tools and algorithms of multi-step data processing, including cleaning, condensation, annotation and analysis, providing powerful data processing support for model training and deployment in multiple fields, including natural language processing and computer vision.Guanaco & QloRAhttps://mp.weixin.qq.com/s/SGJQHsEJTNB6hiVqdc87sghttps://arxiv.org/abs/2305.14314https://github.com/artidoro/qlorahttps://huggingface.co/blog/hf-bitsandbytes-integrationIntegration: https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf?usp=sharingTraining: https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharingWe present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuningon a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4),a new data type that is information theoretically optimal for normally distributed weights (b) Double Quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) Paged Optimizers to manage memory spikes. We use QLoRA to finetunemore than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. We release all of our models and code, including CUDA kernels for 4-bit training.GPT4Allhttps://github.com/nomic-ai/gpt4allDemo, data and code to train an assistant-style large language model with ~800k GPT-3.5-Turbo Generations based on LLaMaHugNLPhttps://mp.weixin.qq.com/s/IpgOQJ8vrIvnjdrmGCT2FAhttps://github.com/HugAILab/HugNLPhttps://arxiv.org/abs/2302.14286华师⼤HugAILab团队研发了HugNLP框架，这是⼀个⾯向研究者和开发者的全⾯统⼀的NLP训练框架，可⽀持包括⽂本分类、⽂本匹配、问答、信息抽取、⽂本⽣成、⼩样本学习等多种NLP任务模型搭建和训练。
HugNLP还集成了⼤量最新的Prompt技术，例如Prompt-Tuning、In-Context Learning、Instruction-tuning，未来还将引⼊Chain-of-thoughtHugAILab团队还研发了⼀系列的应⽤，例如CLUE&GLUE刷榜⼯具，可⽀持ChatGPT类模型训练和部署产品HugChat，以及统⼀信息抽取产品HugIE等。
HugNLP是⼀个分层式框架，遵循“⾼内聚低耦合”的开发模式，其核⼼包括模型层（Models）、处理器层（Processors）、评估器层（Evaluators）和应⽤层（Applications）四部分。
INSTRUCTEVALhttps://mp.weixin.qq.com/s/E6hq0AUy_hItA5HGo2tCAQhttps://github.com/declare-lab/instruct-evalhttps://arxiv.org/abs/2306.04757本⽂引⼊了